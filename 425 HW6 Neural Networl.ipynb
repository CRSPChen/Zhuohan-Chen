{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c313fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a2a6ea",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c634e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random seeds\n",
    "np.random.seed(0)\n",
    "n = 1_000_000\n",
    "x = np.random.normal(0, 1, n) # standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd567eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "y = sigmoid(x)   # 1-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2077222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "def loss_1_layer(weights):\n",
    "    y_pred = sigmoid(weights[0] * x)\n",
    "    return np.mean((y - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4785f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess\n",
    "initial_weights_1_layer = np.array([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d0567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1_layer = minimize(loss_1_layer, initial_weights_1_layer)\n",
    "\n",
    "weights_1_layer_optimized = result_1_layer.x\n",
    "training_error_1_layer = result_1_layer.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296e57a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weight: 0.9999746652072058\n",
      "Training error: 1.521799218766232e-11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Optimized weight: {weights_1_layer_optimized[0]}\")\n",
    "print(f\"Training error: {training_error_1_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2b96492",
   "metadata": {},
   "outputs": [],
   "source": [
    "## When layers euqal to 2\n",
    "def loss_2_layer(weights):\n",
    "    h1 = sigmoid(weights[0] * x)\n",
    "    y_pred = weights[2] * sigmoid(weights[1] * h1)\n",
    "    return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "initial_weights_2_layer = np.array([0.1, 0.1, 0.1])\n",
    "\n",
    "# Minimize the loss function for the 2-layer NN\n",
    "result_2_layer = minimize(loss_2_layer, initial_weights_2_layer)\n",
    "\n",
    "# Extract optimized weights and training error for the 2-layer NN\n",
    "weights_2_layer_optimized = result_2_layer.x\n",
    "training_error_2_layer = result_2_layer.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88c0d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights for 2-layer NN: [5.59337038 3.4605317  0.67314478]\n",
      "Training error for 2-layer NN: 0.010075646134034917\n"
     ]
    }
   ],
   "source": [
    "print(f\"Optimized weights for 2-layer NN: {weights_2_layer_optimized}\")\n",
    "print(f\"Training error for 2-layer NN: {training_error_2_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484ef2d",
   "metadata": {},
   "source": [
    "## Explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58b961",
   "metadata": {},
   "source": [
    "In this case our input and ooutput are simple, the added layer introduced unnecessary complexity, making it harder for the model to learn the simple underlying relationship of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b84be6",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "662fe3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>2.207101</td>\n",
       "      <td>0.112651</td>\n",
       "      <td>1.626798</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>19.872726</td>\n",
       "      <td>2.683904</td>\n",
       "      <td>2.778303</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>2.914857</td>\n",
       "      <td>1.472687</td>\n",
       "      <td>0.218075</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>4.258729</td>\n",
       "      <td>0.242023</td>\n",
       "      <td>0.475822</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>58.108125</td>\n",
       "      <td>0.318110</td>\n",
       "      <td>0.386920</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        distance_from_home  distance_from_last_transaction  \\\n",
       "0                57.877857                        0.311140   \n",
       "1                10.829943                        0.175592   \n",
       "2                 5.091079                        0.805153   \n",
       "3                 2.247564                        5.600044   \n",
       "4                44.190936                        0.566486   \n",
       "...                    ...                             ...   \n",
       "999995            2.207101                        0.112651   \n",
       "999996           19.872726                        2.683904   \n",
       "999997            2.914857                        1.472687   \n",
       "999998            4.258729                        0.242023   \n",
       "999999           58.108125                        0.318110   \n",
       "\n",
       "        ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                             1.945940                1          1   \n",
       "1                             1.294219                1          0   \n",
       "2                             0.427715                1          0   \n",
       "3                             0.362663                1          1   \n",
       "4                             2.222767                1          1   \n",
       "...                                ...              ...        ...   \n",
       "999995                        1.626798                1          1   \n",
       "999996                        2.778303                1          1   \n",
       "999997                        0.218075                1          1   \n",
       "999998                        0.475822                1          0   \n",
       "999999                        0.386920                1          1   \n",
       "\n",
       "        used_pin_number  online_order  fraud  \n",
       "0                     0             0      0  \n",
       "1                     0             0      0  \n",
       "2                     0             1      0  \n",
       "3                     0             1      0  \n",
       "4                     0             1      0  \n",
       "...                 ...           ...    ...  \n",
       "999995                0             0      0  \n",
       "999996                0             0      0  \n",
       "999997                0             1      0  \n",
       "999998                0             1      0  \n",
       "999999                0             1      0  \n",
       "\n",
       "[1000000 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('card_transdata-1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "377de950",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the packages in tutorial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf7a396",
   "metadata": {},
   "source": [
    "Try when max_epochs = 10, lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eed872d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0841\n",
      "Epoch 2/10, Loss: 0.0333\n",
      "Epoch 3/10, Loss: 0.0227\n",
      "Epoch 4/10, Loss: 0.0177\n",
      "Epoch 5/10, Loss: 0.0138\n",
      "Epoch 6/10, Loss: 0.0118\n",
      "Epoch 7/10, Loss: 0.0106\n",
      "Epoch 8/10, Loss: 0.0100\n",
      "Epoch 9/10, Loss: 0.0088\n",
      "Epoch 10/10, Loss: 0.0082\n",
      "Final Accuracy: 0.99796\n",
      "F1 Score: 0.9882745143119899\n"
     ]
    }
   ],
   "source": [
    "# Set X and Y\n",
    "X = df.drop('fraud', axis=1).values\n",
    "y = df['fraud'].values\n",
    "\n",
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "## Basically same as the last homework\n",
    "\n",
    "# Convert to Torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train[:, None], dtype=torch.float32)  # Reshaping for binary classification\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test[:, None], dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define NN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)  # Adjustable\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) # lr rate, set as 0.01\n",
    "\n",
    "# Train the Model\n",
    "num_epochs = 10  # Adjustable\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * inputs.size(0)  # Multiply by batch size\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_preds += labels.size(0)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct_preds / total_preds\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Evaluate the Model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)             ##预测结果\n",
    "    y_pred_class = (y_pred >= 0.5).float()    ##预测类别\n",
    "    accuracy = accuracy_score(y_test_tensor, y_pred_class)\n",
    "    f1 = f1_score(y_test_tensor, y_pred_class)\n",
    "\n",
    "print(f'Final Accuracy: {accuracy}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41e5db",
   "metadata": {},
   "source": [
    "##数据准备和预处理\n",
    "数据集分割：使用train_test_split函数将数据集df分为训练集和测试集，其中test_size=0.2表示测试集占总数据的20%，random_state=42保证每次分割的结果都是一样的。\n",
    "\n",
    "特征标准化：使用StandardScaler对特征进行标准化处理，即使数据的均值为0，标准差为1。这是因为神经网络训练时数值较小且相近的数据可以帮助模型更快地收敛。\n",
    "\n",
    "转换为Torch张量：将NumPy数组转换为PyTorch张量，以便在PyTorch中使用。对于标签y，通过添加一个维度([:, None])来进行重塑，使其适合二分类任务。\n",
    "\n",
    "创建数据加载器：使用DataLoader创建可迭代的数据加载器，用于批量加载数据并在训练期间可选择地对数据进行洗牌。\n",
    "\n",
    "神经网络定义\n",
    "定义网络结构：Net类继承自nn.Module，定义了一个具有三个全连接层（fc1, fc2, fc3）的简单神经网络。网络的第一层接受输入特征的维度，最后一层输出一个单一的预测值。relu作为激活函数增加非线性，最后一层之后应用sigmoid激活函数，输出一个介于0和1之间的预测概率。\n",
    "损失函数和优化器\n",
    "损失函数和优化器：使用二元交叉熵损失（BCELoss）作为损失函数，这适用于二分类问题。SGD（随机梯度下降）作为优化器，负责更新网络的权重。\n",
    "训练过程\n",
    "训练循环：对于指定的训练轮次（num_epochs），执行以下步骤：\n",
    "在每个批次的数据上执行前向传播，计算预测值。\n",
    "计算损失值。\n",
    "执行反向传播，计算梯度。\n",
    "使用优化器更新模型参数。\n",
    "计算并累积整个训练集上的损失和准确率。\n",
    "评估过程\n",
    "模型评估：在训练完成后，将模型设置为评估模式（.eval()），这会停用诸如Dropout等仅在训练时需要的层。使用torch.no_grad()停用梯度计算，减少计算需求和内存使用。\n",
    "计算测试数据的预测值。\n",
    "根据阈值（这里是0.5）将预测概率转换为类别标签。\n",
    "计算并打印准确率和F1分数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e2336",
   "metadata": {},
   "source": [
    "## In this NN,when we add the epoch, the loss becomes lower, and in our final model, the final avvruracy is decent and also the F1 socre, which reprecents this model is trustable, its preformance is also better than a simple random decision tree(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73924ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
